{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert Channels JSON to CSV\n",
    "\n",
    "# import pandas as pd\n",
    "# import json\n",
    "\n",
    "# # Read JSON file\n",
    "# with open('../data/channels.json') as f:\n",
    "#     data = json.load(f)\n",
    "\n",
    "# df = pd.json_normalize(data)\n",
    "\n",
    "# # Rename the columns for clarity\n",
    "# # df.columns = ['id', 'name', 'created', 'creator', 'is_archived', 'is_general', 'members', 'topic_value', 'topic_creator', 'purpose_value', 'purpose_creator']\n",
    "\n",
    "\n",
    "# # Convert JSON to Pandas DataFrame\n",
    "# df = pd.DataFrame(data)\n",
    "\n",
    "# # # Convert Pandas DataFrame to CSV\n",
    "# df.to_csv('../csv/channels.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 'canvases.json' to 'canvases.csv'\n",
      "Converted 'channels.json' to 'channels.csv'\n",
      "Converted 'file_conversations.json' to 'file_conversations.csv'\n",
      "Converted 'integration_logs.json' to 'integration_logs.csv'\n",
      "Converted 'users.json' to 'users.csv'\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# import json\n",
    "# import os\n",
    "\n",
    "# # Specify the directory containing JSON files\n",
    "# json_directory = '../data/'\n",
    "\n",
    "# # Specify the directory to save CSV files\n",
    "# csv_directory = '../csv/'\n",
    "\n",
    "# # Get a list of JSON files in the directory\n",
    "# json_files = [f for f in os.listdir(json_directory) if f.endswith('.json')]\n",
    "\n",
    "# # Process each JSON file and convert it to CSV\n",
    "# for json_file in json_files:\n",
    "#     json_file_path = os.path.join(json_directory, json_file)\n",
    "    \n",
    "#     # Read JSON file\n",
    "#     with open(json_file_path) as f:\n",
    "#         data = json.load(f)\n",
    "\n",
    "#     # Convert JSON to Pandas DataFrame\n",
    "#     df = pd.json_normalize(data)\n",
    "\n",
    "#     # Convert Pandas DataFrame to CSV\n",
    "#     csv_file = os.path.splitext(json_file)[0] + '.csv'  # Use the same file name, but with a CSV extension\n",
    "#     csv_file_path = os.path.join(csv_directory, csv_file)\n",
    "#     df.to_csv(csv_file_path, index=False)\n",
    "\n",
    "#     print(f\"Converted '{json_file}' to '{csv_file}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted '../data2/canvases.json' to '../csv2/.\\canvases.csv'\n",
      "Converted '../data2/channels.json' to '../csv2/.\\channels.csv'\n",
      "Converted '../data2/dms.json' to '../csv2/.\\dms.csv'\n",
      "Converted '../data2/file_conversations.json' to '../csv2/.\\file_conversations.csv'\n",
      "Converted '../data2/groups.json' to '../csv2/.\\groups.csv'\n",
      "Converted '../data2/integration_logs.json' to '../csv2/.\\integration_logs.csv'\n",
      "Converted '../data2/mpims.json' to '../csv2/.\\mpims.csv'\n",
      "Converted '../data2/users.json' to '../csv2/.\\users.csv'\n",
      "Converted '../data2/D05RLN07QS1\\2023-09-10.json' to '../csv2/D05RLN07QS1\\2023-09-10.csv'\n",
      "Converted '../data2/D05RLN07QS1\\2023-09-15.json' to '../csv2/D05RLN07QS1\\2023-09-15.csv'\n",
      "Converted '../data2/D05RLN07QS1\\2023-09-16.json' to '../csv2/D05RLN07QS1\\2023-09-16.csv'\n",
      "Converted '../data2/D05RQM36TK5\\2023-09-10.json' to '../csv2/D05RQM36TK5\\2023-09-10.csv'\n",
      "Converted '../data2/D05RS3C1CQL\\2023-09-20.json' to '../csv2/D05RS3C1CQL\\2023-09-20.csv'\n",
      "Converted '../data2/D05RS3C1CQL\\2023-09-27.json' to '../csv2/D05RS3C1CQL\\2023-09-27.csv'\n",
      "Converted '../data2/D05S3DAUDU1\\2023-09-10.json' to '../csv2/D05S3DAUDU1\\2023-09-10.csv'\n",
      "Converted '../data2/D05S3DAUDU1\\2023-09-11.json' to '../csv2/D05S3DAUDU1\\2023-09-11.csv'\n",
      "Converted '../data2/D05S3DAUDU1\\2023-09-16.json' to '../csv2/D05S3DAUDU1\\2023-09-16.csv'\n",
      "Converted '../data2/D05S3DAUDU1\\2023-09-17.json' to '../csv2/D05S3DAUDU1\\2023-09-17.csv'\n",
      "Converted '../data2/D05S3DAUDU1\\2023-09-18.json' to '../csv2/D05S3DAUDU1\\2023-09-18.csv'\n",
      "Converted '../data2/D05S3DAUDU1\\2023-09-20.json' to '../csv2/D05S3DAUDU1\\2023-09-20.csv'\n",
      "Converted '../data2/D05S3DAUDU1\\2023-09-22.json' to '../csv2/D05S3DAUDU1\\2023-09-22.csv'\n",
      "Converted '../data2/D05S3DAUDU1\\2023-09-26.json' to '../csv2/D05S3DAUDU1\\2023-09-26.csv'\n",
      "Converted '../data2/D05TG4VRWK1\\2023-09-27.json' to '../csv2/D05TG4VRWK1\\2023-09-27.csv'\n",
      "Converted '../data2/D05TG4W0S79\\2023-09-25.json' to '../csv2/D05TG4W0S79\\2023-09-25.csv'\n",
      "Converted '../data2/D05TG4W2TE3\\2023-09-25.json' to '../csv2/D05TG4W2TE3\\2023-09-25.csv'\n",
      "Converted '../data2/D05TG4W2TE3\\2023-09-26.json' to '../csv2/D05TG4W2TE3\\2023-09-26.csv'\n",
      "Converted '../data2/D05TG4W2TE3\\2023-09-27.json' to '../csv2/D05TG4W2TE3\\2023-09-27.csv'\n",
      "Converted '../data2/D05TG4W2TE3\\2023-09-28.json' to '../csv2/D05TG4W2TE3\\2023-09-28.csv'\n",
      "Converted '../data2/D05TG4W2TE3\\2023-10-01.json' to '../csv2/D05TG4W2TE3\\2023-10-01.csv'\n",
      "Converted '../data2/D05TG4W2TE3\\2023-10-03.json' to '../csv2/D05TG4W2TE3\\2023-10-03.csv'\n",
      "Converted '../data2/D05TG4W2TE3\\2023-10-05.json' to '../csv2/D05TG4W2TE3\\2023-10-05.csv'\n",
      "Converted '../data2/D05TG4W2TE3\\2023-10-06.json' to '../csv2/D05TG4W2TE3\\2023-10-06.csv'\n",
      "Converted '../data2/D05TG4W2TE3\\2023-10-08.json' to '../csv2/D05TG4W2TE3\\2023-10-08.csv'\n",
      "Converted '../data2/D05TG4W2TE3\\2023-10-10.json' to '../csv2/D05TG4W2TE3\\2023-10-10.csv'\n",
      "Converted '../data2/D05TG4W2TE3\\2023-10-12.json' to '../csv2/D05TG4W2TE3\\2023-10-12.csv'\n",
      "Converted '../data2/D05TG4W2TE3\\2023-10-13.json' to '../csv2/D05TG4W2TE3\\2023-10-13.csv'\n",
      "Converted '../data2/D05TG4W2TE3\\2023-10-15.json' to '../csv2/D05TG4W2TE3\\2023-10-15.csv'\n",
      "Converted '../data2/D05TN4S07BM\\2023-09-23.json' to '../csv2/D05TN4S07BM\\2023-09-23.csv'\n",
      "Converted '../data2/D05TN7MPDDY\\2023-09-23.json' to '../csv2/D05TN7MPDDY\\2023-09-23.csv'\n",
      "Converted '../data2/D05TQ1PFQ22\\2023-09-27.json' to '../csv2/D05TQ1PFQ22\\2023-09-27.csv'\n",
      "Converted '../data2/D05TQ1PN0DU\\2023-09-25.json' to '../csv2/D05TQ1PN0DU\\2023-09-25.csv'\n",
      "Converted '../data2/D05TQLV4CA0\\2023-09-23.json' to '../csv2/D05TQLV4CA0\\2023-09-23.csv'\n",
      "Converted '../data2/D05TQLV4CA0\\2023-09-26.json' to '../csv2/D05TQLV4CA0\\2023-09-26.csv'\n",
      "Converted '../data2/D05TQLV4CA0\\2023-09-27.json' to '../csv2/D05TQLV4CA0\\2023-09-27.csv'\n",
      "Converted '../data2/D05TQLV4CA0\\2023-09-28.json' to '../csv2/D05TQLV4CA0\\2023-09-28.csv'\n",
      "Converted '../data2/D05TQLV4CA0\\2023-09-29.json' to '../csv2/D05TQLV4CA0\\2023-09-29.csv'\n",
      "Converted '../data2/D05TQLV4CA0\\2023-10-01.json' to '../csv2/D05TQLV4CA0\\2023-10-01.csv'\n",
      "Converted '../data2/D05TQLV4CA0\\2023-10-03.json' to '../csv2/D05TQLV4CA0\\2023-10-03.csv'\n",
      "Converted '../data2/D05TQLV4CA0\\2023-10-09.json' to '../csv2/D05TQLV4CA0\\2023-10-09.csv'\n",
      "Converted '../data2/D05TQLV4CA0\\2023-10-10.json' to '../csv2/D05TQLV4CA0\\2023-10-10.csv'\n",
      "Converted '../data2/D05TQLV4CA0\\canvas_in_the_conversation.json' to '../csv2/D05TQLV4CA0\\canvas_in_the_conversation.csv'\n",
      "Converted '../data2/D05TTNUKDPX\\2023-09-25.json' to '../csv2/D05TTNUKDPX\\2023-09-25.csv'\n",
      "Converted '../data2/D05TWHQQENP\\2023-09-25.json' to '../csv2/D05TWHQQENP\\2023-09-25.csv'\n",
      "Converted '../data2/D05TWHQSF51\\2023-09-27.json' to '../csv2/D05TWHQSF51\\2023-09-27.csv'\n",
      "Converted '../data2/D05TWLL9KH8\\2023-09-27.json' to '../csv2/D05TWLL9KH8\\2023-09-27.csv'\n",
      "Converted '../data2/D05TWLL9KH8\\2023-10-03.json' to '../csv2/D05TWLL9KH8\\2023-10-03.csv'\n",
      "Converted '../data2/D05TWLL9KH8\\2023-10-13.json' to '../csv2/D05TWLL9KH8\\2023-10-13.csv'\n",
      "Converted '../data2/D05TWLL9KH8\\2023-10-14.json' to '../csv2/D05TWLL9KH8\\2023-10-14.csv'\n",
      "Converted '../data2/D05TWLLFXMY\\2023-09-27.json' to '../csv2/D05TWLLFXMY\\2023-09-27.csv'\n",
      "Converted '../data2/D05TZ4DRX5J\\2023-10-06.json' to '../csv2/D05TZ4DRX5J\\2023-10-06.csv'\n",
      "Converted '../data2/D05TZ4DRX5J\\2023-10-07.json' to '../csv2/D05TZ4DRX5J\\2023-10-07.csv'\n",
      "Converted '../data2/D05TZ4DRX5J\\2023-10-09.json' to '../csv2/D05TZ4DRX5J\\2023-10-09.csv'\n",
      "Converted '../data2/D05TZ4DRX5J\\2023-10-13.json' to '../csv2/D05TZ4DRX5J\\2023-10-13.csv'\n",
      "Converted '../data2/D05TZ4DRX5J\\2023-10-15.json' to '../csv2/D05TZ4DRX5J\\2023-10-15.csv'\n",
      "Converted '../data2/D05TZ4E23GC\\2023-09-25.json' to '../csv2/D05TZ4E23GC\\2023-09-25.csv'\n",
      "Converted '../data2/D05U99YMCSD\\2023-09-27.json' to '../csv2/D05U99YMCSD\\2023-09-27.csv'\n",
      "Converted '../data2/D05U99YTRL1\\2023-09-25.json' to '../csv2/D05U99YTRL1\\2023-09-25.csv'\n",
      "Converted '../data2/D05UB7G6NDN\\2023-10-03.json' to '../csv2/D05UB7G6NDN\\2023-10-03.csv'\n",
      "Converted '../data2/D05UB7G6NDN\\2023-10-08.json' to '../csv2/D05UB7G6NDN\\2023-10-08.csv'\n",
      "Converted '../data2/D05UKLESCDN\\2023-09-25.json' to '../csv2/D05UKLESCDN\\2023-09-25.csv'\n",
      "Converted '../data2/D05UKLETLP2\\2023-10-09.json' to '../csv2/D05UKLETLP2\\2023-10-09.csv'\n",
      "Converted '../data2/dev-team\\2023-09-26.json' to '../csv2/dev-team\\2023-09-26.csv'\n",
      "Converted '../data2/dev-team\\2023-09-27.json' to '../csv2/dev-team\\2023-09-27.csv'\n",
      "Converted '../data2/dev-team\\2023-09-28.json' to '../csv2/dev-team\\2023-09-28.csv'\n",
      "Converted '../data2/dev-team\\2023-09-29.json' to '../csv2/dev-team\\2023-09-29.csv'\n",
      "Converted '../data2/dev-team\\2023-09-30.json' to '../csv2/dev-team\\2023-09-30.csv'\n",
      "Converted '../data2/dev-team\\2023-10-01.json' to '../csv2/dev-team\\2023-10-01.csv'\n",
      "Converted '../data2/dev-team\\2023-10-03.json' to '../csv2/dev-team\\2023-10-03.csv'\n",
      "Converted '../data2/dev-team\\2023-10-04.json' to '../csv2/dev-team\\2023-10-04.csv'\n",
      "Converted '../data2/dev-team\\2023-10-05.json' to '../csv2/dev-team\\2023-10-05.csv'\n",
      "Converted '../data2/dev-team\\2023-10-06.json' to '../csv2/dev-team\\2023-10-06.csv'\n",
      "Converted '../data2/dev-team\\2023-10-07.json' to '../csv2/dev-team\\2023-10-07.csv'\n",
      "Converted '../data2/dev-team\\2023-10-08.json' to '../csv2/dev-team\\2023-10-08.csv'\n",
      "Converted '../data2/dev-team\\2023-10-09.json' to '../csv2/dev-team\\2023-10-09.csv'\n",
      "Converted '../data2/dev-team\\2023-10-10.json' to '../csv2/dev-team\\2023-10-10.csv'\n",
      "Converted '../data2/dev-team\\2023-10-11.json' to '../csv2/dev-team\\2023-10-11.csv'\n",
      "Converted '../data2/dev-team\\2023-10-13.json' to '../csv2/dev-team\\2023-10-13.csv'\n",
      "Converted '../data2/dev-team\\2023-10-14.json' to '../csv2/dev-team\\2023-10-14.csv'\n",
      "Converted '../data2/dev-team\\canvas_in_the_conversation.json' to '../csv2/dev-team\\canvas_in_the_conversation.csv'\n",
      "Converted '../data2/FC_F05RSJWSZ1A_https___docs.google.com_spreadsheets_d_1tghhGW2wa4WljNgiWShK-E7ZWkoE-iltCgbs9H9A27Y_edit#gid=495813042\\2023-09-10.json' to '../csv2/FC_F05RSJWSZ1A_https___docs.google.com_spreadsheets_d_1tghhGW2wa4WljNgiWShK-E7ZWkoE-iltCgbs9H9A27Y_edit#gid=495813042\\2023-09-10.csv'\n",
      "Converted '../data2/FC_F05SCLAJUP2_Something that needs to get\\2023-09-10.json' to '../csv2/FC_F05SCLAJUP2_Something that needs to get\\2023-09-10.csv'\n",
      "Converted '../data2/FC_F05TUBQF29M_Important links\\2023-09-27.json' to '../csv2/FC_F05TUBQF29M_Important links\\2023-09-27.csv'\n",
      "Converted '../data2/FC_F061GBBLMQ8_Technical References\\2023-10-11.json' to '../csv2/FC_F061GBBLMQ8_Technical References\\2023-10-11.csv'\n",
      "Converted '../data2/general\\2023-09-10.json' to '../csv2/general\\2023-09-10.csv'\n",
      "Converted '../data2/general\\2023-09-23.json' to '../csv2/general\\2023-09-23.csv'\n",
      "Converted '../data2/general\\2023-09-25.json' to '../csv2/general\\2023-09-25.csv'\n",
      "Converted '../data2/general\\2023-09-27.json' to '../csv2/general\\2023-09-27.csv'\n",
      "Converted '../data2/general\\canvas_in_the_conversation.json' to '../csv2/general\\canvas_in_the_conversation.csv'\n",
      "Converted '../data2/product-management\\2023-09-10.json' to '../csv2/product-management\\2023-09-10.csv'\n",
      "Converted '../data2/product-management\\2023-09-23.json' to '../csv2/product-management\\2023-09-23.csv'\n",
      "Converted '../data2/product-management\\2023-09-25.json' to '../csv2/product-management\\2023-09-25.csv'\n",
      "Converted '../data2/product-management\\2023-09-27.json' to '../csv2/product-management\\2023-09-27.csv'\n",
      "Converted '../data2/proj-ediscovery-by-region\\2023-09-10.json' to '../csv2/proj-ediscovery-by-region\\2023-09-10.csv'\n",
      "Converted '../data2/proj-ediscovery-by-region\\2023-09-11.json' to '../csv2/proj-ediscovery-by-region\\2023-09-11.csv'\n",
      "Converted '../data2/proj-ediscovery-by-region\\2023-09-26.json' to '../csv2/proj-ediscovery-by-region\\2023-09-26.csv'\n",
      "Converted '../data2/proj-ediscovery-by-region\\canvas_in_the_conversation.json' to '../csv2/proj-ediscovery-by-region\\canvas_in_the_conversation.csv'\n",
      "Converted '../data2/random\\2023-09-10.json' to '../csv2/random\\2023-09-10.csv'\n",
      "Converted '../data2/random\\2023-09-23.json' to '../csv2/random\\2023-09-23.csv'\n",
      "Converted '../data2/random\\2023-09-25.json' to '../csv2/random\\2023-09-25.csv'\n",
      "Converted '../data2/random\\2023-09-27.json' to '../csv2/random\\2023-09-27.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Specify the root directory containing JSON files and subdirectories\n",
    "json_root_directory = '../data2/'\n",
    "\n",
    "# Specify the root directory to save CSV files\n",
    "csv_root_directory = '../csv2/'\n",
    "\n",
    "# Function to convert JSON to CSV and preserve directory structure\n",
    "def convert_json_to_csv(json_directory, csv_directory):\n",
    "    for root, _, files in os.walk(json_directory):\n",
    "        for json_file in files:\n",
    "            if json_file.endswith('.json'):\n",
    "                json_file_path = os.path.join(root, json_file)\n",
    "\n",
    "                # Read JSON file\n",
    "                with open(json_file_path, encoding='utf-8') as f:\n",
    "                    data = json.load(f)\n",
    "\n",
    "                # Convert JSON to Pandas DataFrame\n",
    "                df = pd.json_normalize(data)\n",
    "\n",
    "                # Create the CSV directory structure\n",
    "                relative_path = os.path.relpath(root, json_directory)\n",
    "                csv_subdirectory = os.path.join(csv_directory, relative_path)\n",
    "\n",
    "                # Create the CSV subdirectory if it doesn't exist\n",
    "                os.makedirs(csv_subdirectory, exist_ok=True)\n",
    "\n",
    "                # Convert Pandas DataFrame to CSV\n",
    "                csv_file = os.path.splitext(json_file)[0] + '.csv'\n",
    "                csv_file_path = os.path.join(csv_subdirectory, csv_file)\n",
    "                df.to_csv(csv_file_path, index=False)\n",
    "\n",
    "                print(f\"Converted '{json_file_path}' to '{csv_file_path}'\")\n",
    "\n",
    "# Call the function to convert JSON to CSV\n",
    "convert_json_to_csv(json_root_directory, csv_root_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Specify the path of the zip file\n",
    "zip_file_path = '../Kentron Slack export Sep 10 2023 - Oct 15 2023.zip'\n",
    "\n",
    "# Specify the directory to extract the zip file to\n",
    "extract_directory = '../data3/raw/'\n",
    "\n",
    "# Extract the zip file\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_directory)\n",
    "\n",
    "# Get a list of JSON files in the directory\n",
    "json_files = [os.path.join(root, f) for root, _, files in os.walk(extract_directory) for f in files if f.endswith('.json')]\n",
    "\n",
    "# Process each JSON file and convert it to a Pandas DataFrame\n",
    "df_list = []\n",
    "for json_file in json_files:\n",
    "    with open(json_file, encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    df = pd.json_normalize(data)\n",
    "    df_list.append(df)\n",
    "\n",
    "# Concatenate all DataFrames into a single DataFrame\n",
    "df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "csv_file_path = '../data3/csv/normal1.csv'\n",
    "df.to_csv(csv_file_path, index=False)\n",
    "\n",
    "# Convert the DataFrame to a JSON string with proper structure\n",
    "json_string = df.to_json(orient='records', indent=4)\n",
    "\n",
    "# Save the JSON string to a file\n",
    "json_file_path = '../data3/json/normal1.json'\n",
    "with open(json_file_path, 'w') as f:\n",
    "    f.write(json_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'flask_graphql'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32me:\\JUET\\Projects\\kentron-slack-processing\\notebooks\\json_pd_to_csv.ipynb Cell 5\u001b[0m line \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/JUET/Projects/kentron-slack-processing/notebooks/json_pd_to_csv.ipynb#W4sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/JUET/Projects/kentron-slack-processing/notebooks/json_pd_to_csv.ipynb#W4sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mflask\u001b[39;00m \u001b[39mimport\u001b[39;00m Flask, request\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/JUET/Projects/kentron-slack-processing/notebooks/json_pd_to_csv.ipynb#W4sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mflask_graphql\u001b[39;00m \u001b[39mimport\u001b[39;00m GraphQLView\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/JUET/Projects/kentron-slack-processing/notebooks/json_pd_to_csv.ipynb#W4sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mgraphene\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/JUET/Projects/kentron-slack-processing/notebooks/json_pd_to_csv.ipynb#W4sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# Define the GraphQL schema\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'flask_graphql'"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from flask import Flask, request\n",
    "from flask_graphql import GraphQLView\n",
    "import graphene\n",
    "\n",
    "# Define the GraphQL schema\n",
    "class Query(graphene.ObjectType):\n",
    "    pass\n",
    "\n",
    "class UploadZip(graphene.Mutation):\n",
    "    class Arguments:\n",
    "        file = graphene.Upload(required=True)\n",
    "\n",
    "    ok = graphene.Boolean()\n",
    "    csv_path = graphene.String()\n",
    "\n",
    "    def mutate(self, info, file):\n",
    "        # Specify the directory to extract the zip file to\n",
    "        extract_directory = '../data4/raw/'\n",
    "\n",
    "        # Extract the zip file\n",
    "        with zipfile.ZipFile(file, 'r') as zip_ref:\n",
    "            zip_ref.extractall(extract_directory)\n",
    "\n",
    "        # Get a list of JSON files in the directory\n",
    "        json_files = [os.path.join(root, f) for root, _, files in os.walk(extract_directory) for f in files if f.endswith('.json')]\n",
    "\n",
    "        # Process each JSON file and convert it to a Pandas DataFrame\n",
    "        df_list = []\n",
    "        for json_file in json_files:\n",
    "            with open(json_file, encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            df = pd.json_normalize(data)\n",
    "            df_list.append(df)\n",
    "\n",
    "        # Concatenate all DataFrames into a single DataFrame\n",
    "        df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "        # Save the DataFrame to a CSV file\n",
    "        csv_file_path = '../data4/csv/normal1.csv'\n",
    "        df.to_csv(csv_file_path, index=False)\n",
    "\n",
    "        return UploadZip(ok=True, csv_path=csv_file_path)\n",
    "\n",
    "class Mutation(graphene.ObjectType):\n",
    "    upload_zip = UploadZip.Field()\n",
    "\n",
    "schema = graphene.Schema(query=Query, mutation=Mutation)\n",
    "\n",
    "# Create a Flask app\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Add a route for the GraphQL endpoint\n",
    "app.add_url_rule('/graphql', view_func=GraphQLView.as_view('graphql', schema=schema, graphiql=True))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
